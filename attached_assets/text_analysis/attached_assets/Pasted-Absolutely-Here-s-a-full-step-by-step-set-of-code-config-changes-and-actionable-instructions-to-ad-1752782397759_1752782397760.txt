Absolutely! Hereâ€™s a full, step-by-step set of code/config changes and actionable instructions to address your truth analysis performance, resilience, and reliability issues. Each fix includes the file name, code block, and a brief explanation. You can copy-paste these into your project.

---

## 1. **Prompt Length Limiting and Validation**

### server/services/openai.ts
Add a helper to limit input size and enforce a max content length before sending to OpenAI.

```typescript name=server/services/openai.ts
// Add at the top:
const MAX_ANALYSIS_INPUT_LENGTH = 2000; // words or adjust as needed

function trimInputContent(content: string): string {
  if (!content) return "";
  const words = content.split(/\s+/);
  if (words.length > MAX_ANALYSIS_INPUT_LENGTH) {
    return words.slice(0, MAX_ANALYSIS_INPUT_LENGTH).join(" ") + " ...[TRIMMED]";
  }
  return content;
}

// In your analysis function, before sending to OpenAI:
const analyzedContent = trimInputContent(inputContent);
```

---

## 2. **OpenAI Result Caching**

### server/services/openai.ts
Add a simple in-memory cache (for production, use Redis).

```typescript name=server/services/openai.ts
// At the top:
const analysisCache = new Map<string, EnhancedAnalysisResult>();

function getCacheKey(content: string, lengthPreference: string) {
  return `${content}:${lengthPreference}`;
}

// Before making OpenAI API call:
const cacheKey = getCacheKey(analyzedContent, lengthPreference);
if (analysisCache.has(cacheKey)) {
  return analysisCache.get(cacheKey);
}

// After successful OpenAI response:
analysisCache.set(cacheKey, openAIResult); // openAIResult is your parsed response
```
> For Redis, use the `ioredis` or `redis` package and replace the Map usage with Redis get/set.

---

## 3. **API Error Handling & User Feedback**

### server/services/openai.ts
Wrap OpenAI calls in try/catch and return user-friendly errors.

```typescript name=server/services/openai.ts
try {
  // ...OpenAI call
} catch (error: any) {
  // Log detailed error for devs
  console.error("OpenAI Analysis Error:", error);
  // Throw a generic error for the API
  throw new Error(
    error?.message?.includes("quota") 
      ? "Analysis temporarily unavailable due to API quota. Please try again soon."
      : "Analysis failed. Please try again later."
  );
}
```

### client/src/components/enhanced-analysis-results.tsx
Display backend error details to users.

```tsx name=client/src/components/enhanced-analysis-results.tsx
// In your error handling (e.g. catch block):
toast({
  title: "Analysis Error",
  description: error.message || "Analysis failed. Please try again later.",
  variant: "destructive",
});
```

---

## 4. **Frontend/Backend Contract Validation**

### client/src/components/enhanced-analysis-results.tsx
Guard against missing fields before rendering.

```tsx name=client/src/components/enhanced-analysis-results.tsx
// Example for rendering truth analysis:
const ta = analysis?.analysis?.truthAnalysis;
if (!ta) {
  return <div className="text-red-500">Analysis data unavailable. Please retry or contact support.</div>;
}
```

---

## 5. **Model Selection and Toggle**

### server/services/openai.ts
Support both GPT-4o and GPT-4o-mini with a .env toggle.

```typescript name=server/services/openai.ts
const MODEL = process.env.OPENAI_MODEL || "gpt-4o"; // Add to .env.example

// In OpenAI API call:
model: MODEL,
```
```dotenv name=.env.example
# Add or update this line
OPENAI_MODEL=gpt-4o
```

---

## 6. **Remove Console Logging from Production**

### server/index.ts (and other files)
Replace `console.log`/`console.error` with a proper logger.

```typescript name=server/index.ts
// Install and import a logger, e.g. 'pino'
import pino from "pino";
const logger = pino();

// Replace console.log/error:
logger.info("Server started");
logger.error("OpenAI error", error);
```

---

## 7. **Performance Monitoring and Alerts**

### server/services/monitoring.ts (new file)
Add a simple performance monitor.

```typescript name=server/services/monitoring.ts
let recentAnalysisTimes: number[] = [];

export function logAnalysisDuration(ms: number) {
  recentAnalysisTimes.push(ms);
  if (recentAnalysisTimes.length > 100) recentAnalysisTimes.shift();
  if (ms > 10000) {
    // Add alerting logic here, e.g. send email or webhook
    console.warn("Analysis took over 10s!", ms);
  }
}

export function getAvgAnalysisTime() {
  return recentAnalysisTimes.reduce((a,b) => a+b, 0) / (recentAnalysisTimes.length || 1);
}
```
Call `logAnalysisDuration()` after each analysis.

---

## 8. **React Error Boundaries**

### client/src/components/ErrorBoundary.tsx (new file)
Add a React error boundary.

```tsx name=client/src/components/ErrorBoundary.tsx
import React from "react";

class ErrorBoundary extends React.Component {
  state = { hasError: false };
  static getDerivedStateFromError(error: any) {
    return { hasError: true };
  }
  componentDidCatch(error: any, info: any) {
    // Log error to monitoring service if needed
  }
  render() {
    if (this.state.hasError) {
      return <div className="text-red-600">Something went wrong. Please refresh or try again.</div>;
    }
    return this.props.children;
  }
}
export default ErrorBoundary;
```
Wrap your analysis component:
```tsx
<ErrorBoundary>
  <EnhancedAnalysisResults ... />
</ErrorBoundary>
```

---

## 9. **.env Validation at Startup**

### server/index.ts (or server/services/config.ts)
Validate required env keys.

```typescript name=server/services/config.ts
const REQUIRED_ENV_KEYS = ["OPENAI_API_KEY", "OPENAI_MODEL"];
REQUIRED_ENV_KEYS.forEach((key) => {
  if (!process.env[key]) {
    throw new Error(`Missing required env key: ${key}`);
  }
});
```

---

## 10. **Setup Checklist UI (Optional)**

### client/src/components/SetupChecklist.tsx (new file)
Display setup status to the user.

```tsx name=client/src/components/SetupChecklist.tsx
const requiredKeys = ["OPENAI_API_KEY", "OPENAI_MODEL" /* ... */];
export default function SetupChecklist() {
  // Fetch from a /api/env-check endpoint
  // Show green/red for each key
  return <div>Setup checklist (pseudo-code)</div>;
}
```

---

## 11. **Exponential Backoff for API Retries**

### server/services/openai.ts
Retry failed OpenAI calls with backoff.

```typescript name=server/services/openai.ts
async function callOpenAIWithRetry(payload, retries = 3) {
  let delay = 1000;
  for (let i = 0; i < retries; i++) {
    try {
      return await openai.createChatCompletion(payload);
    } catch (e) {
      if (i === retries - 1) throw e;
      await new Promise((res) => setTimeout(res, delay));
      delay *= 2;
    }
  }
}
```

---

## 12. **Limit Input Size in Frontend**

### client/src/components/content-input.tsx (or wherever content is submitted)
Warn users if they paste too much content.

```tsx name=client/src/components/content-input.tsx
const MAX_INPUT_CHARS = 15000;
function handleInputChange(e) {
  if (e.target.value.length > MAX_INPUT_CHARS) {
    toast({
      title: "Input too large",
      description: `Please limit input to ${MAX_INPUT_CHARS} characters.`,
      variant: "warning",
    });
    return;
  }
  // ...set state, etc
}
```

---

## 13. **Documentation Update**

````markdown name=docs/DEV_FIXES_IMPLEMENTED_JULY_2025.md
# Developer Fixes Implemented - July 2025

- Limited input length for truth analysis
- Added backend and frontend input validation
- Implemented OpenAI result caching (in-memory, Redis recommended)
- Improved user-facing error handling for analysis
- Synced frontend/backend contract for analysis results
- Added model selection via .env
- Removed all console logging from production
- Added backend performance monitoring and alerting
- Introduced React error boundaries
- Validated required .env keys at startup
- Added exponential backoff on OpenAI API calls
- Limited input length in frontend as well
````

---

Let me know if you want to see the full code for any of these steps or need further adjustments for your stack!