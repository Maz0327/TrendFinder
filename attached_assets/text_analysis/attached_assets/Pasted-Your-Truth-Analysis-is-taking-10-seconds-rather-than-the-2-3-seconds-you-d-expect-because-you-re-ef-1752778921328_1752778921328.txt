Your Truth Analysis is taking 10+ seconds—rather than the 2–3 seconds you’d expect—because you’re effectively doing multiple heavyweight operations, all in series, every time you run it. Here’s what’s slowing you down and how to fix it:

---

## 1. **Chunked, Sequential OpenAI Calls**

You split the content into N chunks (for long inputs) and then loop over them, calling the OpenAI API once per chunk.

* **Latency hit:** If GPT‑4o‑mini takes \~1 s per request, 5 chunks → \~5 s just for chunk calls, plus overhead for summarization and other calls.
* **Fix:**

  * **Unify into one JSON‐schema prompt** (so you only call the model once for the whole analysis).
  * Or **parallelize** your chunk requests (e.g. `Promise.all`) and stitch together results as they arrive, streaming partials to the UI.

## 2. **Multiple Separate API Calls for Different Analyses**

You likely invoke distinct calls for sentiment, tone, keywords, *then* Truth Analysis. Each call adds its own latency.

* **Latency hit:** 3 calls × 1 s + chunked Truth Analysis = \~8 s+ total.
* **Fix:**

  * **Batch your asks**: Request sentiment, tone, keywords *and* truthAnalysis in one multi-part JSON response.
  * Use system messages to structure a *single* call that returns all fields.

## 3. **Synchronous Database Writes During Analysis**

If you’re persisting each partial result before moving on, those write‑latencies get tacked on.

* **Latency hit:** DB writes can add 50–200 ms each.
* **Fix:**

  * **Defer** writes until after all analysis is done, or batch insert.
  * Or write asynchronously in the background (fire‑and‑forget) so you don’t block the API response.

## 4. **No Streaming to the Front‑end**

You wait for the full analysis before rendering anything. That makes the UI feel sluggish even if the backend is faster.

* **Perceived latency:** Users don’t see *anything* until the last API call returns.
* **Fix:**

  * Enable `stream: true` on your Chat completions.
  * Render each section (fact, observation, insight) as soon as its chunk arrives.

## 5. **Heavy Model for Simple Subtasks**

You use GPT‑4o‑mini for everything—even for short, formulaic parts (keywords, simple summaries).

* **Cost & time hit:** GPT‑4o‑mini is slower than `gpt-3.5-turbo`.
* **Fix:**

  * **Route** simple tasks (keyword extraction, short summaries) to `gpt-3.5-turbo`.
  * Reserve GPT‑4o‑mini only for the multi‑sentence “humanTruth” or “culturalMoment” sections.

---

### Putting It All Together

A streamlined, \~2–3 second flow might look like this:

1. **Single API call** with one big JSON‐schema prompt:

   ```txt
   “Analyze this content and return JSON:
    { sentiment, tone, keywords[], truthAnalysis:{fact, observation, insight, humanTruth, culturalMoment}, cohortOpportunities[] }
    Content: …”
   ```
2. **Stream** the response back and update each UI section immediately.
3. **Batch DB write** only once you’ve received & parsed the full JSON.
4. **Cache** identical analyses for a short TTL so repeat runs are instant.

By collapsing multiple calls into one, parallelizing or streaming as appropriate, and offloading simple work to a faster, cheaper model, you’ll shave 5–8 seconds off your Truth Analysis—and deliver a snappier, more accurate experience.
