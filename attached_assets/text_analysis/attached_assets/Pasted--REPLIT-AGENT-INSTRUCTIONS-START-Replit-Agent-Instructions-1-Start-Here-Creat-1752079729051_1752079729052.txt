<!-- REPLIT_AGENT_INSTRUCTIONS_START -->

## 🧠 Replit Agent Instructions:

1. **Start Here**: Create a new Node.js Repl
2. **First Command**:
```bash
npm init -y && npm install express cors dotenv @supabase/supabase-js openai axios zod
```
3. **Use Port 3000**: Replit expects this
4. **Frontend Path**: Serve static files from `/public`
5. **Environment Variables**: Use Replit Secrets for:
   - SUPABASE_URL
   - SUPABASE_ANON_KEY
   - OPENAI_KEY
   - FORUMSCOUT_API_KEY (optional)

## ❌ Common Pitfalls to Avoid:
- Don't use `import` statements—use `require()`
- Enable CORS for Chrome extension
- Always return JSON from API endpoints
- Add error handling with consistent JSON format

<!-- REPLIT_AGENT_INSTRUCTIONS_END -->

---

## 🚀 Module 0: Project Setup & Structure

<!-- REPLIT_AGENT_PROMPT_START -->

### 🌟 Goal:
Bootstrap the full-stack project structure and baseline routes. Prepare the foundation for all modules. This should run instantly on Replit.

### 🔹 Build Slim:
- Create full folder structure (see below)
- Add `server.js` with Express setup
- Include `.env.example` file
- Serve `/public/index.html` from root
- Add health check route `/api/hello`
- Add Supabase connection client

### 📁 Files to Create:
```
project-root/
├── server.js
├── package.json
├── .env.example
├── /routes
│   ├── auth.js
│   ├── capture.js
│   └── briefs.js
├── /services
│   ├── ai.js
│   ├── supabase.js
│   └── feeds.js
├── /public
│   ├── index.html
│   ├── dashboard.html
│   └── /js
│       ├── app.js
│       └── dashboard.js
├── /extension
│   ├── manifest.json
│   ├── popup.html
│   └── content.js
└── /tests
```

### 📦 Dependencies:
```bash
npm install express cors dotenv @supabase/supabase-js
```

### 📱 API Endpoints:
- GET `/api/hello` returns `{ message: "Hello Strategist!" }`

### 📊 Success Metrics:
- [x] Project boots up
- [x] Folder structure matches above
- [x] /api/hello returns 200 with hello message

### ✨ Optional Build Later:
- Add Swagger/OpenAPI UI
- Add automated schema linter
- Add Husky + Prettier config

### 🤔 Common Mistakes:
- Missing `.env.example`
- Using `import` instead of `require()`
- Forgetting to export Supabase client

<!-- REPLIT_AGENT_PROMPT_END -->

---

## 🚀 Module 1: Auth & Supabase User Setup

<!-- REPLIT_AGENT_PROMPT_START -->

### 🌟 Goal:
Allow users to register/login with email + password using Supabase Auth. Set up `users` table and RLS policies.

### 🔹 Build Slim:
- POST `/api/auth/register` + `/login`
- GET `/api/auth/user` to fetch session
- Store basic user info in Supabase `users` table

### 📁 Files to Create:
- `/routes/auth.js`
- `/services/supabase.js`

### 📦 Dependencies:
Already included: `@supabase/supabase-js`

### 📱 API Endpoints:
- POST `/api/auth/register` { email, password }
- POST `/api/auth/login` { email, password }
- GET `/api/auth/user` returns session info

### 📂 Database SQL:
```sql
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email TEXT UNIQUE NOT NULL,
  created_at TIMESTAMP DEFAULT NOW()
);
```

### 📊 Success Metrics:
- [x] User can register and log in
- [x] Supabase session is stored
- [x] GET `/api/auth/user` returns correct user

### 🚗 Dependencies:
- Requires: Module 0
- Blocks: Module 2, 4, 6

### 📌 Build Later:
- Magic Link login
- Role-based permissions
- OAuth (Google login)

<!-- REPLIT_AGENT_PROMPT_END -->

---

## 🚀 Module 2: Chrome Extension Capture

<!-- REPLIT_AGENT_PROMPT_START -->

### 🌟 Goal:
Enable users to capture content from any webpage using a Chrome Extension and send it to the backend. Supports selected text or full-page capture.

### 🔹 Build Slim:
- Build basic Chrome Extension UI (popup.html + popup.js)
- Allow user to capture selected text or entire page
- Send content to backend via `/api/capture`

### 📁 Files to Create:
- `/extension/manifest.json`
- `/extension/popup.html`
- `/extension/popup.js`
- `/extension/content.js`
- `/routes/capture.js`
- `/services/supabase.js` (reuse)

### 📦 Dependencies:
Already installed: `express`, `cors`, `dotenv`, `@supabase/supabase-js`

### 📱 API Endpoints:
- POST `/api/capture`
Body:
```json
{
  "url": "https://reddit.com/r/marketing/post123",
  "title": "Why TikTok marketing is changing",
  "captured_text": "Marketers are noticing that...",
  "tags": ["tiktok", "marketing"]
}
```
Returns:
```json
{
  "success": true,
  "signal_id": "uuid-here",
  "message": "Captured successfully"
}
```

### 📂 Database SQL:
```sql
CREATE TABLE signals (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  url TEXT,
  title TEXT,
  captured_text TEXT,
  summary TEXT,
  sentiment TEXT,
  tone TEXT,
  keywords TEXT[],
  tags TEXT[],
  status TEXT DEFAULT 'captured',
  created_at TIMESTAMP DEFAULT NOW()
);

ALTER TABLE signals ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users see own signals" ON signals
  FOR ALL USING (auth.uid() = user_id);
```

### 📊 Success Metrics:
- [x] User can capture content from web
- [x] POST `/api/capture` stores it in Supabase
- [x] Response contains success + ID

### 🚗 Dependencies:
- Requires: Module 1
- Blocks: Module 3, 4

### 📌 Build Later:
- Add platform detection (Reddit, TikTok, etc)
- Add keyboard shortcut capture
- Offline queuing
- Screenshot capture

<!-- REPLIT_AGENT_PROMPT_END →

What do you think about this?
---
## 🧠 Module 3: AI Signal Summarization & Enhancement
<!-- REPLIT_AGENT_PROMPT_START -->
### 🎯 Goal:
Build an endpoint and processing service to analyze captured content (from Module 2) using an AI model (OpenAI by default), summarize it, extract strategic attributes, and store the enhanced signal in the database.
### 📌 Why:
Strategists need fast, contextual summaries of content to determine its strategic value. Manually reviewing raw captures is slow and inconsistent. This module automates signal enrichment.
### 🧱 Build Slim:
- AI summary endpoint
- Basic OpenAI integration (GPT-4 Turbo default)
- Extract tone, sentiment, keywords
- Store AI-enhanced summary in signals table
### 🔮 Build Later:
- Multi-model routing (Gemini, Claude, etc.)
- Style-tuned prompt templates
- Advanced keyword classification
- Summarization length presets
- Auto-tagging from user-defined vocabularies
### 📦 Dependencies:
```bash
npm install openai zod
```
### 📁 Deliverables:
- /routes/ai.js – API endpoint for AI summarization
- /services/ai.js – LLM integration and prompt building
- /prompts/summarizePrompt.txt – Customizable system prompt
### 📡 API Endpoints:
```http
POST /api/ai/summarize
Body: {
signal_id: string
}
Returns: {
summary: string,
tone: string,
sentiment: string,
keywords: string[]
}
```
### 📝 Use SignalSchema from contracts.js
- Enforce structure on response before writing to DB
### 🧪 TEST PLAN:
- Send a captured signal ID → receive enriched fields
- Check DB for updated summary/metadata fields
- Handle edge case: missing capture, token limit exceeded, model failover
### 🧠 AI Prompt Logic:
```js
const buildPrompt = (captured_text, options = {}) => `
You are an insight assistant helping strategists.
Summarize the following captured content, extract tone, sentiment, and 5 strategic keywords.
Content:
"""
${captured_text}
"""
Respond in JSON:
{
summary: "...",
tone: "...",
sentiment: "...",
keywords: ["...", "...", "..."]
}`;
```
### ✅ Success Metrics:
- [ ] Can enrich a signal with AI summary
- [ ] Stores tone, sentiment, and keywords
- [ ] Errors handled gracefully if model fails
### 🔗 Dependencies:
- Requires: Module 2 (captured signals)
- Blocks: Module 4 (dashboard visualization), Module 6 (brief building)
### 📋 Build Later Backlog:
- Add LLM Router to switch providers based on content type/length
- Add user settings for prompt tone/style
- Implement AI cost monitoring + provider fallback
<!-- REPLIT_AGENT_PROMPT_END →

---

## 📊 Module 4: Signal Dashboard & Management UI

<!-- REPLIT_AGENT_PROMPT_START -->

### 🎯 Goal:
Build a frontend dashboard (dashboard.html) and backend route to view, manage, and filter all captured and AI-enhanced signals. Should allow basic tagging, status updates, and filtering based on topic or metadata.

### 📌 Why:
Strategists need to browse, organize, and manage the raw and enriched data in a clean, filterable interface to make strategic sense of saved signals.

### 🧱 Build Slim:
- List signals by created_at
- Filter by status (captured, enriched, etc.)
- Inline tag editor
- Update signal status (e.g., to potential, signal)
- Simple search by title/text

### 🔮 Build Later:
- Smart filters by sentiment, tone, keywords
- Save user-defined filters
- Sorting by AI score or user rating
- Calendar view / timeline clustering

### 📦 Dependencies:
- Vanilla JS + HTML
- Supabase client (browser)
- Optional: Tailwind for styling

### 📁 Deliverables:
- `/public/dashboard.html`
- `/public/js/dashboard.js`
- `/routes/signals.js`

### 📡 API Endpoints:
```http
GET    /api/signals?status=captured
PUT    /api/signals/:id
Body: { status?: string, tags?: string[] }
```

### 🧪 TEST PLAN:
- Load dashboard and view all signals
- Change signal status and confirm update
- Add/remove tags inline
- Filter by signal type

### ✅ Success Metrics:
- [ ] Signals load correctly from backend
- [ ] Status and tags are editable
- [ ] Filters update the signal list accurately

### 🔗 Dependencies:
- Requires: Module 3 (enriched signals)
- Blocks: Module 6 (brief building)

### 📋 Build Later Backlog:
- Create drag/drop board for pipeline views
- Support starred/favorite signals
- Include AI confidence rating sort

<!-- REPLIT_AGENT_PROMPT_END →

...

<!-- REPLIT_AGENT_PROMPT_START -->

## 🧩 Module 5: Topics of Conversation Feed (API Integration)

### 🎯 Build Slim:
- Connect to **Google Trends** and **Reddit** APIs to fetch daily trending topics.
- Normalize incoming data to a shared `TopicSchema`.
- Display top 5 topics on the dashboard.

### 🧱 Build Later:
- Add **LinkedIn (via ForumScout)**, **TikTok Display API**, **YouTube Trending**, **Spotify Charts**, and **News API**.
- Score topics for relevance using keyword heuristics.
- Allow filtering by platform and export to CSV.

### 📁 Files to Create:
- `/routes/topics.js` – REST API to serve fetched topics
- `/services/fetchTopics.js` – Source fetchers and aggregation logic
- `/schemas/topicSchema.js` – Zod or JSON Schema validator
- `/tests/fetchTopics.test.js` – API integration tests

### 📦 Dependencies:
```bash
npm install axios rss-parser zod
```

### 📡 API Endpoints:
- `GET /api/topics`
  - Query Params: `platform=reddit|google`
  - Response: `TopicSchema[]`

### 🗄️ Database (Optional in Build Slim):
```sql
CREATE TABLE topics (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  platform TEXT,
  title TEXT,
  summary TEXT,
  url TEXT,
  score NUMERIC,
  fetched_at TIMESTAMP DEFAULT NOW()
);
```

### 📝 Shared Contracts:
```js
// schemas/topicSchema.js
export const TopicSchema = {
  platform: z.enum(['reddit', 'google', 'linkedin', 'youtube', 'tiktok']),
  title: z.string(),
  summary: z.string().optional(),
  url: z.string().url(),
  score: z.number().optional(),
};
```

### 🧪 Test Plan:
- [ ] `GET /api/topics?platform=reddit` returns 5 valid topics
- [ ] Schema validates with `TopicSchema`
- [ ] Network failures fallback to cached data

### 📊 Success Metrics:
- Build Slim: At least 5 trending topics fetched and shown from 2 platforms
- Build Later: Support for 5 total platforms and export working

### 🔗 Dependencies:
- Requires: Module 1 (Auth)
- Blocks: Module 6 (Brief Builder)

### 📋 Future Backlog:
- Add NLP keyword extraction to each topic
- Integrate with signal scoring engine
- Add timeline trend graphs per topic

### 🧠 Replit Agent Instructions:
<!-- REPLIT_AGENT_HINT:
- Fetch RSS or JSON data from Reddit and Google Trends
- Normalize response to `TopicSchema`
- Store in memory or Supabase table
- Use `/routes/topics.js` to serve API
- Validate response against `TopicSchema`
-->

<!-- REPLIT_AGENT_PROMPT_END -->
---

<!-- REPLIT_AGENT_PROMPT_START -->

## 🧩 Module 6: Brief Builder

### 🎯 Build Slim:
- Allow users to select signals and generate a text brief in a simple textarea UI.
- Export brief as plain text.

### 🧱 Build Later:
- Add export formats: PDF, Word, Markdown.
- Integrate prompt templates with AI rephrasing.
- Style brief using Vayner-style formatting.
- Allow saving multiple versions of a brief.

### 📁 Files to Create:
- `/routes/briefs.js` – API to generate and retrieve briefs
- `/public/brief.html` – Simple UI for brief writing
- `/services/briefBuilder.js` – Generate brief content from selected signals
- `/tests/briefs.test.js`

### 📦 Dependencies:
```bash
npm install jsdom file-saver
```

### 📡 API Endpoints:
- `POST /api/brief`
  - Body: `{ selectedSignals: SignalSchema[] }`
  - Returns: `{ briefText: string }`
- `GET /api/briefs/:id`
  - Returns: `{ id, briefText, created_at }`

### 📝 Use `SignalSchema` from contracts.js to validate input.

### 📥 Deliverables:
- Brief editor UI with textarea
- Export as `.txt` with download button

### 🗄️ Database (Optional in Slim):
```sql
CREATE TABLE briefs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  brief_text TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);
```

### 🧪 Test Plan:
- [ ] POST with 2 test signals returns generated brief
- [ ] Generated brief is downloadable
- [ ] Brief validates business logic (summary + context)

### 📊 Success Metrics:
- Build Slim: User can generate and export at least 1 brief
- Build Later: 3 export formats supported and AI refinement integrated

### 🔗 Dependencies:
- Requires: Module 1 (Auth), Module 4 (Dashboard)
- Blocks: Module 7 (Optional Analytics)

### 📋 Future Backlog:
- Multi-language brief support
- Tone customization (casual vs. strategic)
- Feedback loop from strategist edits

### 🧠 Replit Agent Instructions:
<!-- REPLIT_AGENT_HINT:
- Generate brief from selected signals using string join/summarize logic
- No need for AI in Build Slim unless already integrated
- Export as `.txt` using FileSaver or HTML5 blob
- Validate signal array using `SignalSchema`
-->

<!-- REPLIT_AGENT_PROMPT_END →

---

<!-- REPLIT_AGENT_PROMPT_START -->

## 📈 Module 7: Optional Analytics & Logging

### 🎯 Build Slim:
- Track basic app usage (signal saves, brief exports) in console logs or simple in-memory store.

### 🧱 Build Later:
- Full analytics dashboard for internal use.
- Export analytics data.
- Store usage logs in Supabase.
- Add filters by user/session/date.

### 📁 Files to Create:
- `/services/analytics.js` – Logging utility
- `/routes/analytics.js` – Optional API to fetch logs
- `/tests/analytics.test.js`

### 📦 Dependencies:
_No new dependencies in Slim. Use built-in `console.log` or Node in-memory store._

```bash
# Build Later may require:
npm install lowdb
```

### 📡 API Endpoints (Build Later):
- `GET /api/analytics/usage`  
  Returns aggregated analytics stats.

- `POST /api/analytics/event`
  Body: `{ type: string, user_id: UUID, metadata: object }`

### 📥 Deliverables:
- Console-based logging of:
  - Capture events
  - AI analysis events
  - Brief exports

### 🗄️ Database (Build Later):
```sql
CREATE TABLE analytics_events (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  event_type TEXT,
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);
```

### 🧪 Test Plan:
- [ ] Confirm capture event logs to console
- [ ] POST event creates new row in Supabase (Build Later)
- [ ] GET endpoint returns filtered stats (Build Later)

### 📊 Success Metrics:
- Build Slim: Console logs all 3 event types
- Build Later: Full dashboard exportable by admin

### 🔗 Dependencies:
- Requires: Module 1 (Auth)
- Optional: All other modules for richer context

### 📋 Future Backlog:
- Heatmap visualization of user activity
- Admin dashboard filters by user/date
- Export logs to CSV or Google Sheets

### 🧠 Replit Agent Instructions:
<!-- REPLIT_AGENT_HINT:
- For Build Slim: log to console with timestamps
- Structure: { eventType, userId, metadata, timestamp }
- Build Later: insert into Supabase and expose via `/api/analytics/usage`
-->

<!-- REPLIT_AGENT_PROMPT_END -->






