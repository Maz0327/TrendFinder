It's a common issue to get blocked or rate-limited when using `pytrends` because it's an unofficial library that scrapes Google's web pages. Google's servers can detect and temporarily block IP addresses that make too many automated requests in a short time (often with an HTTP 429 "Too Many Requests" error).

Here are the most effective strategies to avoid getting blocked.

-----

### 1\. Introduce Delays

The simplest way to mimic human behavior and avoid detection is to pause between your requests. This prevents you from hitting the server too frequently.

You can add a short, random delay using Python's `time` and `random` libraries.

```python
import time
import random
from pytrends.request import TrendReq

pytrends = TrendReq(hl='en-US', tz=360)
kw_list = ["Tesla", "Ford", "General Motors"]

for keyword in kw_list:
    pytrends.build_payload([keyword], cat=0, timeframe='today 5-y', geo='US')
    interest_df = pytrends.interest_over_time()
    print(interest_df.head())
    
    # Wait for a random time between 2 and 5 seconds before the next request
    time.sleep(random.uniform(2, 5))
```

-----

### 2\. Use Proxies

Using proxies routes your requests through different IP addresses. This is a very effective method because it makes your requests appear to be coming from various users instead of a single server.

You can find both free and paid proxy services online. Paid services are generally more reliable.

The `pytrends` library allows you to pass a proxy configuration directly to it.

```python
from pytrends.request import TrendReq

# Example proxies (replace with your actual proxies)
proxies = {
    'http': 'http://user:pass@123.45.67.89:1234',
    'https': 'https://user:pass@123.45.67.89:1234',
}

# Connect using the proxy
pytrends = TrendReq(hl='en-US', tz=360, proxies=[proxies['https'], proxies['http']])

# Now build your payload and make requests as usual
kw_list = ["AI"]
pytrends.build_payload(kw_list)
print(pytrends.interest_over_time())
```

-----

### 3\. Customize Request Headers

Sometimes, specifying a common browser `User-Agent` can help your requests look more like legitimate browser traffic. You can do this by passing a custom `requests` session object to `pytrends`.

```python
import requests
from pytrends.request import TrendReq

# Create a requests session
session = requests.session()

# Set a common user-agent header
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'
})

# Initialize pytrends with the custom session
pytrends = TrendReq(hl='en-US', tz=360, requests_args={'session': session})

# Proceed with your requests
kw_list = ["sustainability"]
pytrends.build_payload(kw_list)
print(pytrends.interest_over_time())
```

By combining these methods, particularly **delays** and **proxies**, you can significantly reduce the likelihood of your system being blocked.