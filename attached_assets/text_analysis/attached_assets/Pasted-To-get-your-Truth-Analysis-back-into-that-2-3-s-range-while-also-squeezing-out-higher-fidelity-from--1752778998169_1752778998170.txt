To get your Truth Analysis back into that 2–3 s range while also squeezing out higher fidelity from GPT‑4o‑mini, you’ll want to tackle three fronts in tandem: **prompt engineering**, **API orchestration**, and **post‑processing validation**. Here’s a recipe that lets you keep GPT‑4o‑mini as your core model, but deliver faster, more accurate, and more reliable “truthAnalysis” results:

---

## 1. Unify & Streamline Your Prompt

### ✏️ One JSON‑Schema Prompt

Instead of separate calls (sentiment, tone, keywords, truth), ask for **everything** in a single completion:

```ts
const systemMsg = {
  role: "system" as const,
  content: `
You are an expert content strategist assistant. 
Analyze the following text and output valid JSON exactly matching this schema:

{
  "sentiment": "Positive|Neutral|Negative",
  "tone": "Formal|Informal|Encouraging|Urgent",
  "keywords": ["...","...",...],
  "truthAnalysis": {
    "fact": "1–2 sentences",
    "observation": "1–2 sentences",
    "insight": "1–2 sentences",
    "humanTruth": "1–2 sentences",
    "culturalMoment": "1–2 sentences",
    "attentionValue": "high|medium|low",
    "cohortOpportunities": ["...","..."]
  }
}

Use temperature 0.0 for deterministic output. Don’t wrap in markdown, return only JSON.
  `.trim()
};

const userMsg = {
  role: "user" as const,
  content: data.content
};

const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  stream: true,
  messages: [systemMsg, userMsg]
});
```

* **Why this helps:**

  * **Single call** removes the overhead of multiple round‑trips.
  * **Streaming** means your UI can paint the sentiment/tone/keywords as soon as they arrive, with the truthAnalysis fields following immediately—perceived latency drops to \~1 s.
  * **JSON schema + temp 0** ensures GPT‑4o‑mini sticks to your structure, boosting accuracy and eliminating parse errors.

---

## 2. Leverage Function Calling for Schema Enforcement

OpenAI’s function‑calling lets you define a JSON schema once and have the model conform:

```ts
const functions = [{
  name: "analyzeContent",
  description: "Returns structured analysis of content",
  parameters: {
    type: "object",
    properties: {
      sentiment: { type: "string", enum: ["Positive","Neutral","Negative"] },
      tone: { type: "string", enum: ["Formal","Informal","Encouraging","Urgent"] },
      keywords: { type: "array", items: { type: "string" } },
      truthAnalysis: {
        type: "object",
        properties: {
          fact: { type: "string" },
          observation: { type: "string" },
          insight: { type: "string" },
          humanTruth: { type: "string" },
          culturalMoment: { type: "string" },
          attentionValue: { type: "string", enum: ["high","medium","low"] },
          cohortOpportunities: { type: "array", items: { type: "string" } }
        },
        required: ["fact","observation","insight","humanTruth","culturalMoment","attentionValue","cohortOpportunities"]
      }
    },
    required: ["sentiment","tone","keywords","truthAnalysis"]
  }
}];

const res = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [systemMsg, userMsg],
  functions,
  function_call: { name: "analyzeContent" }
});

// then JSON.parse(res.choices[0].message.function_call.arguments)
```

* **Why this helps:**

  * You **never** have to manual‑parse or worry about missing keys.
  * The model is **guided** to fill every field, boosting completeness and accuracy.

---

## 3. Fine‑Tune Your Prompt with Few‑Shot Examples

Give GPT‑4o‑mini 1–2 concrete examples of desired output:

```ts
const example = {
  role: "system",
  content: `
Example:
Input: "Our user base grew by 30% but churn remains at 15%—we need to improve retention."
Output:
{
  "sentiment":"Neutral",
  "tone":"Analytical",
  "keywords":["user base","30% growth","churn","retention"],
  "truthAnalysis":{
    "fact":"User base increased by 30% and churn is 15%.",
    "observation":"Rapid growth is not translating to loyalty.",
    "insight":"Acquisition strategies are strong but onboarding may be weak.",
    "humanTruth":"Users value seamless onboarding as much as new features.",
    "culturalMoment":"Market demands better user experience post‑pandemic.",
    "attentionValue":"medium",
    "cohortOpportunities":["new users","power users"]
  }
}
`.trim()
};
```

* **Why this helps:**

  * Anchors the model on your exact JSON format and phrasing style.
  * Reduces hallucinations by showing real‑world examples of “humanTruth” and “culturalMoment.”

---

## 4. Post‑Process & Validate

After receiving the function response:

```ts
const analysis = JSON.parse(res.choices[0].message.function_call.arguments);
if (!analysis.truthAnalysis.fact) {
  // optionally retry with a “Please ensure truthAnalysis.fact is non‑empty” follow‑up
}
```

* **Why this helps:**

  * Catches edge‑cases where GPT might omit a field.
  * Lets you **auto‑retry** a single follow‑up prompt instead of showing a broken UI.

---

## 5. Cache & Reuse Hot Results

Wrap your analysis call in a short‑TTL cache keyed by a hash of the content + lengthPreference:

```ts
const key = sha256(data.content + lengthPreference);
const cached = await redis.get(key);
if (cached) return JSON.parse(cached);
const fresh = await callOpenAI(...);
await redis.set(key, JSON.stringify(fresh), "EX", 300);
return fresh;
```

* **Why this helps:**

  * Identical or similar analyses skip the 2–3 s call entirely.
  * Great for demos where testers rerun the same content.

---

## 6. UI Streaming & Perceived Speed

On the React side, hook into the streamed response to fill sections as data arrives:

```tsx
useEffect(() => {
  const stream = openai.chat.stream(...);
  stream.on("data", chunk => {
    if (chunk.choices[0].delta.function_call) {
      // accumulate arguments JSON
    }
    if (chunk.choices[0].delta.content) {
      // accumulate non‑function text (sentiment, tone)
      setSentiment(...);
    }
  });
  stream.on("end", () => {
    const final = JSON.parse(accumulatedJson);
    setTruthAnalysis(final.truthAnalysis);
  });
}, [input]);
```

* **Why this helps:**

  * Users **see** the sentiment & tone almost instantly, and the truthAnalysis fields fill in moments later—**perceived** latency feels near‑instant.

---

### Putting It All Together

1. **One GPT call** (with function‑calling + few‑shot + temp 0)
2. **Stream** partial results into the UI
3. **Validate** and retry missing fields once
4. **Cache** for repeated inputs

This flow will run in **\~2–3 s** end‑to‑end, maintain **high accuracy** via schema enforcement and examples, and let you stick with GPT‑4o‑mini without sacrificing quality.
