Great—that means the media analyzer is alive. The clean next move is to wire those analysis results into captures with proper API endpoints and persistence, so the rest of the app (and later the UI) can rely on a stable contract.

Here’s a single copy-paste block for Replit:

⸻

Replit Task Block #5 — Capture ↔️ Media Analysis Integration (API + DB + Tests)

Goal:
Attach analysis output to captures in a first-class table, expose clean endpoints to upload/trigger analysis, fetch results, and (optionally) auto-tag captures based on analysis. No UI changes.

0) Safety/Context
	•	Don’t modify existing routes or env that are working.
	•	Keep using MEDIA_PROVIDER=google, ENABLE_WORKERS=true, AUTO_TAG_FROM_ANALYSIS=true.
	•	All client calls must go through the server’s /api/*. No direct Supabase from the client.

⸻

1) DB migration (table + indexes + RLS)

Create migration SQL at server/migrations/20250814_capture_analyses.sql with:

-- Table to store per-capture analysis snapshots
create table if not exists public.capture_analyses (
  id uuid primary key default gen_random_uuid(),
  capture_id uuid not null references public.captures(id) on delete cascade,
  provider text not null check (provider in ('google','openai','mock')),
  mode text not null check (mode in ('sync','deep')),
  status text not null default 'completed',    -- 'queued' | 'processing' | 'completed' | 'failed'
  summary text,
  labels jsonb,        -- normalized tags/labels [{name, conf, source}], can be empty array
  ocr jsonb,           -- text blocks w/ positions
  transcript text,     -- for videos (ASR)
  keyframes jsonb,     -- [{ts, url}] if we extract thumbnails
  raw jsonb,           -- raw provider response (trimmed)
  error text,
  created_at timestamptz not null default now()
);

create index if not exists idx_capture_analyses_capture_id on public.capture_analyses(capture_id);
create index if not exists idx_capture_analyses_created_at on public.capture_analyses(created_at desc);

-- RLS
alter table public.capture_analyses enable row level security;

-- Policies: user can see only analyses for their own captures
drop policy if exists "read own capture analyses" on public.capture_analyses;
create policy "read own capture analyses"
on public.capture_analyses for select
using (
  exists (
    select 1 from public.captures c
    where c.id = capture_id and c.user_id = auth.uid()
  )
);

-- Only server-side service role writes; if you already have a service role key configured,
-- keep insert/update/delete to service role only (no direct client writes).
drop policy if exists "no client writes to capture_analyses" on public.capture_analyses;
create policy "no client writes to capture_analyses"
on public.capture_analyses for all
to authenticated
using (false) with check (false);

Apply the migration using your existing method. If CLI/service key is unavailable, HUMAN STOP: paste the SQL in Supabase SQL Editor and run it, then continue.

⸻

2) Server: routes + service layer

Add routes in server/routes/analysis.ts (new file):

import { Router } from "express";
import type { Request, Response } from "express";
import { requireAuth } from "./middleware"; // reuse your auth middleware
import { analysisService } from "../services/analysis"; // you'll create this
const r = Router();

// Upload + create capture + analyze (for Chrome extension / file uploads)
r.post("/captures/upload-and-analyze", requireAuth, analysisService.uploadAndAnalyze);

// Trigger (re)analysis for an existing capture (decides sync vs deep)
r.post("/captures/:id/analyze", requireAuth, analysisService.analyzeExisting);

// Get latest analysis for a capture
r.get("/captures/:id/analysis", requireAuth, analysisService.getLatestForCapture);

// Optional: check job status by id
r.get("/analysis/:jobId", requireAuth, analysisService.getJobStatus);

export default r;

Register in server/routes.ts:

import analysisRoutes from "./routes/analysis";
app.use("/api", analysisRoutes);

Service implementation server/services/analysis.ts:
	•	Use existing Google Gemini provider you wired.
	•	Respect ANALYSIS_MAX_SYNC_IMAGE_BYTES: if file > threshold, enqueue deep analysis; else run sync.
	•	Persist into public.capture_analyses with normalized fields.
	•	If AUTO_TAG_FROM_ANALYSIS=true, merge top N labels into captures.tags (dedupe, lowercase).

Skeleton:

import type { Request, Response } from "express";
import { db } from "../db"; // your existing db access or supabase server client
import { storage } from "../storage"; // existing storage helper (bucket 'media')
import { queue } from "../queue"; // if you exposed a queue; else inline logic for deep path
import { analyzeImageSync, analyzeImageDeep } from "../providers/google"; // you already have provider functions

function normalizeLabels(raw: any): { name: string; conf?: number; source?: string }[] {
  // extract top labels safely; return []
}

async function writeAnalysisRow(data: {
  capture_id: string; provider: string; mode: "sync"|"deep";
  status: string; summary?: string; labels?: any; ocr?: any; transcript?: string;
  keyframes?: any; raw?: any; error?: string;
}) {
  // insert into public.capture_analyses via server-side supabase client
}

async function maybeAutoTagCapture(captureId: string, labels: {name:string}[]) {
  if (process.env.AUTO_TAG_FROM_ANALYSIS !== "true") return;
  // load capture.tags, merge label names (lowercase), write back
}

export const analysisService = {
  uploadAndAnalyze: async (req: Request, res: Response) => {
    // accept multipart/form-data: file + {project_id?, title?, source_url?}
    // 1) store file in 'media' bucket, path by user/captureId
    // 2) create capture row (status 'processing' optional)
    // 3) if size <= threshold -> sync analyze; else enqueue deep analyze
    // 4) persist analysis row (status completed or queued)
    // 5) maybeAutoTagCapture
    // 6) return { captureId, analysisId?, status }
  },

  analyzeExisting: async (req, res) => {
    // look up capture, locate media file, choose sync/deep by size, run or enqueue
    // write analysis row, maybe auto-tag, return status
  },

  getLatestForCapture: async (req, res) => {
    // select latest analysis for capture by created_at desc and return
  },

  getJobStatus: async (req, res) => {
    // if you have a queue/job id, return status; else 404
  }
};

Reuse your existing Google provider functions & storage helpers. Keep the code minimal; this is a thin orchestration layer.

⸻

3) Auto-tagging mapping (simple, opinionated)

Create server/services/analysis-tag-map.ts:

export function toTags(labels: { name: string }[], limit = 8): string[] {
  const raw = labels.map(l => l.name.toLowerCase().trim());
  const mapped = raw.map(s => s.replace(/\s+/g, "-")); // kebab
  // optionally map synonyms: e.g. "x (twitter)" -> "twitter"
  const dedup = Array.from(new Set(mapped));
  return dedup.slice(0, limit);
}

Use this in maybeAutoTagCapture.

⸻

4) OpenAPI (contract for UI)

Update or create server/openapi.json with:
	•	POST /api/captures/upload-and-analyze (multipart form data, returns {captureId, analysisId?, status})
	•	POST /api/captures/{id}/analyze (JSON {mode?: "sync"|"deep"} returns status)
	•	GET /api/captures/{id}/analysis (returns latest analysis object)
	•	GET /api/analysis/{jobId} (optional)

Keep descriptions short; this is mainly for Bolt/UI wiring.

⸻

5) Smoke tests

Create scripts/smoke-analysis.ts:
	•	Upload a small test image (embed a tiny PNG in the script as base64 to avoid external files).
	•	Call POST /api/captures/upload-and-analyze with the file.
	•	Poll GET /api/captures/:id/analysis until status="completed" (with timeout).
	•	Assert labels array present; if AUTO_TAG_FROM_ANALYSIS=true, confirm capture.tags includes at least one label.

Add npm script:

"scripts": {
  "smoke:analysis": "ts-node --transpile-only scripts/smoke-analysis.ts"
}

Run it and print PASS/FAIL.

⸻

6) Verification (print this)
	•	✅ Migration applied (table + indexes + RLS)
	•	✅ Routes live (/api/captures/upload-and-analyze, /api/captures/:id/analysis, etc.)
	•	✅ Sync vs Deep path honored by ANALYSIS_MAX_SYNC_IMAGE_BYTES
	•	✅ Analysis rows persisted; raw trimmed
	•	✅ Auto-tag wrote back to captures.tags (if enabled)
	•	✅ Smoke test PASS with sample image

If anything fails, print the failing step and the log excerpt.

⸻

This lands a durable, backend-first contract for analysis tied to captures. Once in, we can move to Block #6: expose read-only summary fields on GET /api/captures (e.g., latest_analysis.summary, labels[]) so the UI can show insights without extra calls.