Replit Agent — Supabase Schema Recovery, Drift Check, Safe Seed, and Types
Context: You may have previously changed the database schema during seeding. From now on, do not auto-alter schema. Follow these steps exactly.

0) Guardrails (must follow)
Do not run any CREATE/ALTER/DROP directly.

If schema drift is found, output a single SQL migration file proposal at supabase/migrations/#######_reconcile.sql and stop for human review.

Only run seed inserts after schema matches spec.

Never expose the service role key to the browser.

1) Expected schema (source of truth)
Target tables/columns must match this spec (names + nullability + types). Use it to compare:

public.users

id: uuid PK default gen_random_uuid()

email: text not null unique

username: text

role: text

onboarding_completed: boolean

tour_completed: boolean

progress_data: jsonb

google_tokens: jsonb

created_at: timestamptz not null default now()

updated_at: timestamptz not null default now()

public.captures

id: uuid PK default gen_random_uuid()

user_id: uuid not null references public.users(id) on delete cascade

title: text not null

content: text not null

platform: text

url: text

tags: text[]

viral_score: int

ai_analysis: jsonb

dsd_tags: text[]

dsd_section: text

predicted_virality: numeric

actual_virality: numeric

created_at: timestamptz not null default now()

updated_at: timestamptz not null default now()

public.cultural_moments

id: uuid PK default gen_random_uuid()

title: text not null unique

description: text not null

intensity: int not null

platforms: text[]

demographics: text[]

duration: text

created_at: timestamptz not null default now()

updated_at: timestamptz not null default now()

public.dsd_briefs

id: uuid PK default gen_random_uuid()

user_id: uuid not null references public.users(id) on delete cascade

client_profile_id: uuid

title: text not null

status: text

define_section: jsonb

shift_section: jsonb

deliver_section: jsonb

created_at: timestamptz not null default now()

updated_at: timestamptz not null default now()

Also ensure touch_updated_at() trigger exists on all four tables to keep updated_at fresh.

2) Schema drift check (read-only)
Run these SQL queries and print results to the console. Do not modify anything yet.

sql
Copy
Edit
-- List columns (name, type, nullable, default) for each table
with cols as (
  select 
    c.table_schema,
    c.table_name,
    c.column_name,
    c.is_nullable,
    c.data_type,
    c.udt_name,
    c.ordinal_position,
    pgd.description as column_comment,
    pgc.relname as relname
  from information_schema.columns c
  left join pg_catalog.pg_class pgc
    on pgc.relname = c.table_name
  left join pg_catalog.pg_namespace pgn
    on pgn.oid = pgc.relnamespace and pgn.nspname = c.table_schema
  left join pg_catalog.pg_attribute pga
    on pga.attrelid = pgc.oid and pga.attname = c.column_name
  left join pg_catalog.pg_description pgd
    on pgd.objoid = pga.attrelid and pgd.objsubid = pga.attnum
  where c.table_schema = 'public'
    and c.table_name in ('users','captures','cultural_moments','dsd_briefs')
)
select * from cols order by table_name, ordinal_position;
sql
Copy
Edit
-- Primary keys
select tc.table_name, kc.column_name
from information_schema.table_constraints tc
join information_schema.key_column_usage kc
  on kc.constraint_name = tc.constraint_name
where tc.constraint_type = 'PRIMARY KEY'
  and tc.table_schema = 'public'
  and tc.table_name in ('users','captures','cultural_moments','dsd_briefs')
order by tc.table_name, kc.ordinal_position;
sql
Copy
Edit
-- Unique constraints
select tc.table_name, tc.constraint_name, kc.column_name
from information_schema.table_constraints tc
join information_schema.key_column_usage kc
  on kc.constraint_name = tc.constraint_name
where tc.constraint_type = 'UNIQUE'
  and tc.table_schema = 'public'
  and tc.table_name in ('users','captures','cultural_moments','dsd_briefs')
order by tc.table_name, tc.constraint_name, kc.ordinal_position;
sql
Copy
Edit
-- Foreign keys
select
  tc.table_name, tc.constraint_name, kcu.column_name,
  ccu.table_name as foreign_table_name,
  ccu.column_name as foreign_column_name,
  rc.update_rule, rc.delete_rule
from information_schema.table_constraints tc
join information_schema.key_column_usage kcu
  on tc.constraint_name = kcu.constraint_name and tc.table_schema = kcu.table_schema
join information_schema.referential_constraints rc
  on rc.constraint_name = tc.constraint_name and rc.constraint_schema = tc.table_schema
join information_schema.constraint_column_usage ccu
  on ccu.constraint_name = tc.constraint_name and ccu.constraint_schema = tc.table_schema
where tc.constraint_type = 'FOREIGN KEY'
  and tc.table_schema = 'public'
  and tc.table_name in ('users','captures','cultural_moments','dsd_briefs')
order by tc.table_name, tc.constraint_name;
sql
Copy
Edit
-- Check updated_at triggers
select event_object_table as table_name, trigger_name, action_timing, action_statement
from information_schema.triggers
where trigger_schema = 'public'
  and event_object_table in ('users','captures','cultural_moments','dsd_briefs')
order by event_object_table, trigger_name;
Compare the outputs against the spec in step 1. If anything differs (missing columns, wrong types, missing PK/unique/FK, missing triggers), proceed to step 3.

3) Generate a reconciliation migration (do not execute)
Create supabase/migrations/<timestamp>_reconcile.sql that only contains the SQL required to align the live DB with the spec. Examples (use only what’s needed):

sql
Copy
Edit
-- Example fixes (edit as needed)

-- Add missing column
alter table public.captures add column if not exists predicted_virality numeric;

-- Change type safely (use using clause if needed)
alter table public.captures
  alter column viral_score type integer using (viral_score::integer);

-- Add missing FK
alter table public.captures
  add constraint captures_user_id_fkey
  foreign key (user_id) references public.users(id) on delete cascade;

-- Add unique on users.email
do $$
begin
  if not exists (
    select 1 from pg_constraint where conname = 'users_email_key'
  ) then
    alter table public.users add constraint users_email_key unique (email);
  end if;
end $$;

-- Recreate/ensure updated_at trigger + function
create or replace function public.touch_updated_at()
returns trigger language plpgsql as $$
begin
  new.updated_at = now();
  return new;
end $$;

do $$
begin
  if not exists (select 1 from pg_trigger where tgname = 'touch_users_updated_at') then
    create trigger touch_users_updated_at before update on public.users
    for each row execute function public.touch_updated_at();
  end if;
  if not exists (select 1 from pg_trigger where tgname = 'touch_captures_updated_at') then
    create trigger touch_captures_updated_at before update on public.captures
    for each row execute function public.touch_updated_at();
  end if;
  if not exists (select 1 from pg_trigger where tgname = 'touch_cultural_moments_updated_at') then
    create trigger touch_cultural_moments_updated_at before update on public.cultural_moments
    for each row execute function public.touch_updated_at();
  end if;
  if not exists (select 1 from pg_trigger where tgname = 'touch_dsd_briefs_updated_at') then
    create trigger touch_dsd_briefs_updated_at before update on public.dsd_briefs
    for each row execute function public.touch_updated_at();
  end if;
end $$;
After writing the file, STOP and print:
“Migration proposal written to supabase/migrations/<timestamp>_reconcile.sql. Awaiting approval to apply.”

4) (Human approves) Apply migration
Only after explicit human approval, run the contents of that migration in Supabase SQL editor (or via Supabase CLI). Then continue.

5) Safe seed (idempotent; data only)
Implement/update scripts/seed.ts to insert sample rows without altering schema. Use UPSERTs where sensible and minimal datasets. Example:

ts
Copy
Edit
// scripts/seed.ts
import 'dotenv/config'
import { createClient } from '@supabase/supabase-js'

const supabase = createClient(
  process.env.VITE_SUPABASE_URL as string,
  process.env.SUPABASE_SERVICE_ROLE_KEY as string // server-side only
)

async function main() {
  // users
  const { data: user, error: uErr } = await supabase
    .from('users')
    .upsert([{ email: 'demo@trendfinder.ai', username: 'demo', role: 'admin' }], { onConflict: 'email' })
    .select()
    .single()
  if (uErr) throw uErr

  // captures
  const { error: cErr } = await supabase.from('captures').insert([
    {
      user_id: user.id,
      title: 'Sample TikTok trend',
      content: 'Example content about a rising meme format...',
      platform: 'tiktok',
      url: 'https://tiktok.com/@example/video/123',
      tags: ['meme','trend'],
      viral_score: 72,
      ai_analysis: { summary: 'High momentum', hooks: ['duet this', 'remix this'] },
      dsd_tags: ['define:culture','deliver:creative-beta'],
      dsd_section: 'define',
      predicted_virality: 0.63,
      actual_virality: null
    }
  ])
  if (cErr && !String(cErr.message).includes('duplicate')) throw cErr

  console.log('Seed completed.')
}

main().catch((e) => {
  console.error('Seed failed:', e?.message || e)
  process.exit(1)
})
Add npm script:

json
Copy
Edit
{
  "scripts": {
    "seed": "tsx scripts/seed.ts",
    "schema:check": "tsx scripts/schema-check.ts || true"
  }
}
(If tsx isn’t installed dev-dep, install it; or write the seed in plain JS and run with node.)

6) Regenerate TypeScript types (client)
After schema is reconciled, run:

bash
Copy
Edit
npx supabase gen types typescript --project-id $SUPABASE_PROJECT_REF --schema public > src/types/supabase.ts
If CLI isn’t available in this environment, skip and do not attempt to auto-install—print a message telling the human to run it locally and push src/types/supabase.ts.

7) Success criteria
Drift report printed.

Migration proposal file written (no direct DB changes by you).

After human approval and application, npm run seed succeeds.

Types regenerated or human instructed to regenerate.

No schema writes occur from seed; only data inserts.

